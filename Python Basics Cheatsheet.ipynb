{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter8 - Text Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!!      is great, isn't it? So is     !!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 1: Text Preprocessing (Removing punctuation and digits)\n",
    "s = \"Hello!! 1984 is great, isn't it? So is 2018!!!\"\n",
    "import string\n",
    "puncs = string.punctuation # strings !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "dgts = string.digits #strings 0123456789\n",
    "table_p = str.maketrans(puncs, len(puncs) * \" \") #make a mapping relation\n",
    "table_d = str.maketrans(dgts, len(dgts) * \" \")\n",
    "print(s.translate(table_d))# remove all digits with \" \"\n",
    "string.printable # all printable digits & charactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Getting the word frequency & Removing stopwords using nltk\n",
    "#2.1 Getting the word frequency\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "with open('uber_s1.txt') as f: # f \n",
    "    words = nltk.word_tokenize(f.read().lower()) #f.read() is str, nltk.word_tokenize split words, return lst\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(words) # take in a list, return a dict\n",
    "c.most_common(20) # return a list of tupple\n",
    "\n",
    "freq = nltk.FreqDist(words) #dict\n",
    "print(freq) \n",
    "freq.plot(20)\n",
    "\n",
    "#2.2 Removing stopwords using nltk\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "stopwords = nltk.corpus.stopwords.words('english') # return a list \n",
    "print(type(stopwords), len(stopwords), stopwords, sep=\"\\n\")\n",
    "\n",
    "words2 = [] # our accumulator list\n",
    "for w in words:\n",
    "    if w not in stopwords and len(w) > 1:\n",
    "        words2.append(w) # return a clean list\n",
    "# list comprehension\n",
    "words3 = [w for w in words if w not in stopwords and len(w) > 1]\n",
    "\n",
    "#plot the clean list\n",
    "freq2 = nltk.FreqDist(words2)\n",
    "print('common words without stop words')\n",
    "freq2.plot(20);\n",
    "\n",
    "#2.3 Word cloud visualization with wordcloud\n",
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "text = open('uber_s1.txt').read() # string\n",
    "wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "plt.figure(figsize=(10,10)) # set up figure size\n",
    "plt.imshow(wordcloud) # word cloud image show\n",
    "plt.axis(\"on\") # turn on axis\n",
    "plt.savefig('my_word_cloud.png') # save as PNG file\n",
    "plt.savefig('my_word_cloud.pdf') # save as PDF file\n",
    "plt.show()  # show in Jupyter notebook\n",
    "\n",
    "# for a clean list\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "text2 = '' # our string accumulator\n",
    "for word in text.split():\n",
    "    if len(word) == 1 or word in stopwords:\n",
    "        continue\n",
    "    text2 = text2 + ' ' + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3 Sentiment analysis using TextBlob module\n",
    "#3.1 Sentiment Analysis using TextBlob\n",
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "s = 'BAIT 508 is a great class'\n",
    "tb = TextBlob(s) # take in string. transform the the text into a tb object. \n",
    "print(tb.sentiment) # Sentiment(polarity=0.8, subjectivity=0.75)\n",
    "tb.sentiment.polarity\n",
    "tb.sentiment.subjectivity\n",
    "#3.2 sentiment on a news article\n",
    "from textblob import TextBlob\n",
    "with open('data/uber_article.txt') as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "tb = TextBlob(content)\n",
    "sentences = tb.sentences # to split the sentences of a text, return a list of tb sentences (tb objects)\n",
    "\n",
    "#check some high polarity sentences\n",
    "for s in sentences:\n",
    "    tb = TextBlob(str(s)) # take in sentence (need to be transformed back to string)\n",
    "    pol = tb.sentiment.polarity\n",
    "    if abs(pol) > 0.4:\n",
    "        print(pol)\n",
    "        print(s)\n",
    "        print()\n",
    "#collect sentiment scores for each sentence\n",
    "sub_list = []\n",
    "pol_list = []\n",
    "\n",
    "for s in sentences:\n",
    "    tb = TextBlob(str(s))\n",
    "    sub_list.append(tb.sentiment.subjectivity)\n",
    "    pol_list.append(tb.sentiment.polarity)\n",
    "    \n",
    "#visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sub_list, bins=10) #, normed=1, alpha=0.75)\n",
    "\n",
    "plt.xlabel('subjectivity score')\n",
    "plt.ylabel('sentence count')\n",
    "plt.grid(True)\n",
    "plt.savefig('subjectivity.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pol_list, bins=10) #, normed=1, alpha=0.75)\n",
    "\n",
    "plt.xlabel('polarity score')\n",
    "plt.ylabel('sentence count')\n",
    "plt.grid(True)\n",
    "plt.savefig('polarity.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4 POS & NER\n",
    "#4.1 Tokenization and POS tagging with nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "with open('data/uber_article.txt') as infile:\n",
    "    article = infile.read()\n",
    "sentences = nltk.sent_tokenize(article) # input: text string, return list of string sentences.\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences] # return a list of list of words\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] # return a list of list of tupples)\n",
    "\n",
    "#Check the tags:\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('DT')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "\n",
    "#4.2 Spacy with NER\n",
    "!pip install spacy\n",
    "import spacy\n",
    "!python3 -m spacy download en\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "with open('data/uber_s1.txt') as infile:\n",
    "    article = infile.read()\n",
    "doc = nlp(article)\n",
    "doc.ents\n",
    "\n",
    "# create an empty defaultdict\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print('label', ent.label_, '\\ttext', ent.text)\n",
    "    ner_categories[ent.label_] += 1\n",
    "    #print(ent.label_, ent.text)    \n",
    "ner_categories\n",
    "\n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "#values = list(ner_categories.values())\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(values, \n",
    "        labels=labels, \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter9 Data Science Management"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 What is Data Science?\n",
    "#From Wikipedia:\n",
    "#A multi-disciplinary field that:\n",
    "#Uses scientific methods, processes, algorithms, and systems\n",
    "#To extract knowledge and insights\n",
    "#From structured and unstructured data\n",
    "\n",
    "#2 Why now? Big Data + Powerful Computers\n",
    "\n",
    "#3 Data science workflow:\n",
    "#Data collection : Customer survey, Clickstream data, Sales, transactions\n",
    "#Data exploration and visualization : Dashboards, One-off reports\n",
    "#Prediction model and experimentation: Hypothesis testing with A/B testing, Build machine learning models for prediction\n",
    "\n",
    "#4 Building Data Science Team\n",
    "#Data engineer: Control data flow, information architecture, build storage solutions, maintain data access, Use SQL, Java/Scala/Python to process data and automate\n",
    "#Data analyst: Create dashboards, hypothesis testing, data visualization, Use Spreadsheets (Excel), SQL for large-scale analysis, BI tools (Tableau, Power BI, Looker)\n",
    "#ML scientist: Build prediction models, classifications (e.g., stock price prediction, image processing, sentiment analysis), Usually need CS-like background. Use Python and R\n",
    "\n",
    "#5 Data collection and storage\n",
    "#Data storage and databases:\n",
    "#Parallel storage solutions: Multiple computers in a cluster on premise (“on-prem”); On the Cloud: Microsoft Azure, Amazon Web Services, Google Cloud\n",
    "#Types of storage: Unstructured data: Email, text, videos, audios, web pages, social media, stored in “document database” (e.g., MongoDB); Tabular (Structured) data: relational database (e.g., MySQL, Oracle)\n",
    "#Databases: Document database: NoSQL (not only SQL), Relational database: SQL (structured query language)\n",
    "#Decision points: Physical location (on-prem, cloud)? Data types?\n",
    "\n",
    "#6 Machine Learning Concepts\n",
    "#What is Machine Learning (ML)?\n",
    "Use data to develop statistical models that can be used to predict various outcomes for new data\n",
    "vs. codifying human knowledge w/ explicit instructions (for loop, if condition, etc.)\n",
    "A.k.a. predictive modeling, data mining, predictive analytics\n",
    "An example: spam filtering\n",
    "Predict if an email is spam or not\n",
    "Datasets: features and labels\n",
    "Features: Data that might predict the label (e.g., email addr, domain, text)\n",
    "Labels: the correct answers to learn from (e.g., spam or not)\n",
    "#Supervised machine learning\n",
    "Predictions from data with “features” (A) and “labels” (B)\n",
    "Learning A-to-B mapping\n",
    "Examples:\n",
    "Given an email, predict if it is spam or not (spam detection)\n",
    "Given a Facebook photo, identify the persons in it (face recognition)\n",
    "Given a customer and behavior, predict if the customer will churn or not\n",
    "Algorithms:\n",
    "Classification (labels=discrete values): logistic regression, support vector machines, k-nearest neighbors, etc.\n",
    "Regression (labels=continuous values): linear regression, random forest, etc.\n",
    "Some algorithms can do both classification and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2\n",
    "!pip install textblob --user\n",
    "#1. Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "sent = 'BAIT 508 is a great class'\n",
    "tb = TextBlob(sent)\n",
    "# sentiment scores\n",
    "print(tb.sentiment)\n",
    "# polarity score\n",
    "print(tb.sentiment.polarity)\n",
    "# subjectivity score\n",
    "print(tb.sentiment.subjectivity)\n",
    "s_neg = 'this is not a great class!!!'\n",
    "tb_neg = TextBlob(s_neg)\n",
    "print(tb_neg.sentiment)\n",
    "tb = TextBlob('I was like happy and stuff')\n",
    "tb.sentiment\n",
    "\n",
    "#2. Language translation with TextBlob\n",
    "s = 'BAIT 508 is a great class. Sauder MBAN students are awesome!'\n",
    "tb = TextBlob(s)\n",
    "print(tb.detect_language())\n",
    "import time\n",
    "\n",
    "s = 'BAIT 508 is a great class. Sauder MBAN students are awesome!'\n",
    "tb = TextBlob(s)\n",
    "\n",
    "lst_language = ['ko', 'cs', 'es', 'zh', 'hi', 'id', 'ru', 'ar', 'th', 'vi', 'ja']\n",
    "for lang in lst_language:\n",
    "    print(tb.translate(to=lang))\n",
    "    time.sleep(1)\n",
    "\n",
    "#3. Spell correction with TextBlob\n",
    "b = TextBlob(\"I havv goood speling!\")\n",
    "print(b.correct())\n",
    "b = TextBlob(\"I m trying mi best to make prfect spellling\")\n",
    "print(b.correct())\n",
    "\n",
    "#4. Text summarization.\n",
    "!pip install gensim --user\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization import summarize\n",
    "with open('uber_article.txt') as f:\n",
    "    uber_text = f.read()\n",
    "print(len(uber_text), 'characters in the text')\n",
    "print(summarize(uber_text))\n",
    "print(summarize(uber_text, ratio=0.05))\n",
    "print(keywords(uber_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter7 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 Import flat files\n",
    "#1.1 Exploring your working directory\n",
    "!ls\n",
    "#1.2 Importing entire text files\n",
    "# Open a file: file\n",
    "file = open('data/moby_dick.txt', mode='r')\n",
    "\n",
    "#check charactors\n",
    "with open(\"feed_header_2017-2019.tsv\", \"r\") as infile:\n",
    "    text = infile.read()\n",
    "ans1 = int(len(text))\n",
    "print(ans1)\n",
    "\n",
    "#check words\n",
    "ans2 = int(len(text.split()))\n",
    "print(ans2)\n",
    "\n",
    "#check lines: \n",
    "with open(\"feed_header_2017-2019.tsv\", \"r\") as infile:\n",
    "    lines = infile.readlines()\n",
    "ans3 = int(len(lines))\n",
    "print(ans3)\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n",
    "\n",
    "#1.3 Writing a text file\n",
    "outfile = open('bait508_hw1.txt', 'w')\n",
    "outfile.write('Lee, Gene, email@gmail.com\\n')\n",
    "outfile.write('ans1=100\\n')\n",
    "outfile.write('ans2=200\\n')\n",
    "outfile.close()\n",
    "\n",
    "#1.4 Use context manager with and import text files line by line\n",
    "with open('data/moby_dick.txt') as file:\n",
    "    print(file.readline())\n",
    "    print(file.readline())\n",
    "    print(file.readline())\n",
    "    \n",
    "#1.5 Go over a huge file line by line\n",
    "with open('data/blockchain.txt') as file:\n",
    "    for line in file: # this won't load the whole text into the RAM\n",
    "        print(line[0:80]) # print the first 80 characters in the line\n",
    "\n",
    "#1.6 \n",
    "!head data/mnist_test.csv\n",
    "!head data/seaslug.txt\n",
    "!head data/titanic.tsv\n",
    "\n",
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'data/titanic.tsv'\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "# View the head of the DataFrame\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2 Import Flat files from Web\n",
    "#2.1 Importing flat files from the web\n",
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "df.head()\n",
    "\n",
    "#2.2 Opening and reading flat files from web (without storing locally)\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep=';')\n",
    "# Print the head of the DataFrame\n",
    "df.head()\n",
    "# Plot first column of df\n",
    "pd.DataFrame.hist(df.iloc[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "####2.3 Parsing webpage with requests and BeautifulSoup!!\n",
    "#2.1 Download webpage with requests\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.sauder.ubc.ca/thought-leadership/divisions/accounting-information-systems/people'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "type(r)\n",
    "#2.2 Parse the text with BeautifulSoup\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "print(type(r))\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "print(soup)\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)\n",
    "\n",
    "# let's check webpage title\n",
    "print(soup.title)\n",
    "\n",
    "# let's now check webpage text\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "with open('web_text.txt', 'w') as outfile:\n",
    "    outfile.write(text)\n",
    "    \n",
    "# Find all 'div' tags <div class=\"profile\">\n",
    "div_tags = soup.find_all('div', {'class': 'profile'})\n",
    "print(type(div_tags))\n",
    "print(div_tags)\n",
    "print(len(div_tags))\n",
    "\n",
    "# if you want to find tags without specific parameters condition, \n",
    "# div_tags = soup.find_all('div')\n",
    "\n",
    "for tag in div_tags:\n",
    "    #print(link)\n",
    "    #try:\n",
    "    name = tag.find('h4').get_text().strip()\n",
    "    email = tag.find('div', {'class': 'profile__content__position'}).get_text().strip()\n",
    "    print(name, email)\n",
    "    #except:\n",
    "    #    continue\n",
    "\n",
    "# Import package\n",
    "import pandas as pd\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "xl = pd.read_excel(url, sheet_name=None)\n",
    "print(xl.keys())\n",
    "print(xl['1700'].head())\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Standard_Industrial_Classification\"\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "tables = soup.find_all('table')\n",
    "first_table = tables[0]\n",
    "trs = first_table.find_all('tr')\n",
    "sic_name = dict()\n",
    "for tr in trs:\n",
    "    try:\n",
    "        tds = tr.find_all('td')\n",
    "        k = tds[0].get_text().strip()\n",
    "        v = tds[1].get_text().strip()\n",
    "        sic_name[k] = v\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "sic_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3 API & JSON\n",
    "#1. Introduction to APIs and JSONs\n",
    "import json\n",
    "from pprint import pprint \n",
    "\n",
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "print(type(json_data))\n",
    "\n",
    "for k, v in json_data.items():\n",
    "    print(k + ': ', v)\n",
    "    \n",
    "#2. APIs and interacting with WWW\n",
    "# Import requests package\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "print(r.text)\n",
    "\n",
    "#2.2 Convert requests response to JSON\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "print(type(json_data))\n",
    "\n",
    "pprint(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter6 Pandas + EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1. Data frame basics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "filepath = \"1st_class.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "df\n",
    "df.shape\n",
    "df.columns\n",
    "df.info()\n",
    "df.index\n",
    "df.isnull().sum()\n",
    "df.dropna(axis=1, how=\"all\", inplace = True)\n",
    "df.fillna(\"\", inplace = True)\n",
    "df.rename(columns={'pup':'pop'},inplace=True)\n",
    "df.iloc[:5,:]\n",
    "df.tail()\n",
    "filepath = \"1st_class.csv\"\n",
    "df_country = pd.read_csv(filepath, index_col='country')\n",
    "df_country.rename(columns={'pup':'pop'},inplace=True)\n",
    "df_country.index\n",
    "df_canada = df_country.loc['Canada',:]\n",
    "df_canada.head(5)\n",
    "df_canada['continent'].str.upper()\n",
    "df_canada[df_canada['continent'].str.contains('A')]\n",
    "del df_canada['continent']\n",
    "df_canada.head(5)\n",
    "df_canada.describe()\n",
    "df_canada['pop'].count()\n",
    "df_canada['lifeExp'].mean()\n",
    "df_canada['gdpPercap'].std()\n",
    "df_canada['gdpPercap'].quantile(0.75) - df_canada['gdpPercap'].quantile(0.25)\n",
    "plt.plot(df_canada.year, df_canada.lifeExp, label=\"Canada\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Life Expectancy\")\n",
    "plt.title(\"Life Expectancy in Canada\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "df_us = df[df['country']=='United States']\n",
    "plt.plot(df_canada.year, df_canada.lifeExp, label='Canada')\n",
    "plt.plot(df_us.year, df_us.lifeExp, label='US')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Life Expectancy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Life Expectancy in Canada and US\")\n",
    "plt.show()\n",
    "df_canada.corr()\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.matshow(df_canada.corr(), cmap=plt.get_cmap('summer'), fignum=1)\n",
    "plt.xticks(range(df_canada.shape[1]), df_canada.columns, fontsize=10, rotation=0)\n",
    "plt.yticks(range(df_canada.shape[1]), df_canada.columns, fontsize=10, rotation=0)\n",
    "plt.colorbar(orientation=\"vertical\")\n",
    "plt.show()\n",
    "\n",
    "star_review = yelp_final_filter[['stars','review_count']]\n",
    "star_review.corr()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.matshow(star_review.corr(), cmap='gray', fignum=1)\n",
    "plt.colorbar(orientation=\"vertical\")\n",
    "plt.xticks(range(star_review.shape[1]), star_review.columns, fontsize=10, rotation=0)\n",
    "plt.yticks(range(star_review.shape[1]), star_review.columns, fontsize=10, rotation=0)\n",
    "plt.title(\"Correlation matrix between stars and review count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2. Concarteneation Merge\n",
    "df = pd.concat([df_1, df_2], ignore_index=False)\n",
    "df.tail()\n",
    "#ignore_index = True means it will clear the existing index and reset it\n",
    "df = pd.concat([df_1, df_2], ignore_index=True, axis = 1)\n",
    "df.head(2)\n",
    "#axis=1 means it will concatenate along the columns.\n",
    "#axis=0 means it will concatenate along the rows.\n",
    "df_continent_left = pd.merge(df, continent, on=['country'], how='left')\n",
    "df_continent_left\n",
    "df_continent_right = pd.merge(df, continent, on=['country'], how='right')\n",
    "df_continent_right\n",
    "df_continent = pd.merge(df, continent, on=['country'], how='left', indicator=True)\n",
    "df_continent.head(2)\n",
    "df_continent['_merge'].unique()\n",
    "#Subset\n",
    "df_continent['country']=='Canada'\n",
    "df_canada = df_continent[df_continent['country']=='Canada']\n",
    "df_canada = df_continent[df_continent['country'].isin(['Canada'])]\n",
    "df_canada['no_meaning']=1\n",
    "#Groupby\n",
    "df_continent.groupby('country').count()\n",
    "df_continent.groupby('country').sum()\n",
    "df_grouped=df_continent.groupby([\"country\",\"year\"]).agg(\"mean\")\n",
    "df_continent.groupby(['country','year']).mean().index\n",
    "df_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter5 loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 For Loop\n",
    "#1. for loop patterns\n",
    "#1.1 Iteration loop pattern\n",
    "words = ['stop', 'desktop', 'post', 'top']\n",
    "for word in words:\n",
    "    if 'top' in word:\n",
    "        print(word)\n",
    "#1.2 Counter Loop\n",
    "s = 'hello'\n",
    "for i, c in enumerate(s):\n",
    "    print(i, c)\n",
    "#1.3Acumulator loop pattern\n",
    "import time\n",
    "\n",
    "lst = [3, 2, 7, 1, 9]\n",
    "\n",
    "res = 0\n",
    "for num in lst:\n",
    "    print('{} + {} = {}'.format(res, num, res+num))\n",
    "    res = res + num\n",
    "    time.sleep(1)\n",
    "print(res)\n",
    "\n",
    "#2 continue and break statements\n",
    "table = [\n",
    "    [2, 3, 0, 6],\n",
    "    [0, 3, 4, 5],\n",
    "    [4, 5, 6, 0]]\n",
    "for row in table:\n",
    "    for item in row:\n",
    "        print(item, end=' ')\n",
    "    print()\n",
    "    \n",
    "for row in table:\n",
    "    for num in row:\n",
    "        if num == 0:\n",
    "            break   # break out of the current for loop\n",
    "        print(num, end=' ')\n",
    "    print()\n",
    "\n",
    "for row in table:\n",
    "    for num in row:\n",
    "        if num == 0:\n",
    "            continue # continue with the current for loop\n",
    "        print(num, end=' ')\n",
    "    print()\n",
    "    \n",
    "# While loop:\n",
    "#1.1 basic while loop pattern\n",
    "import time\n",
    "\n",
    "i = 7\n",
    "while i <= 37:\n",
    "    i += 7\n",
    "    print('in while loop, i=', i)\n",
    "    time.sleep(1)\n",
    "print(i)\n",
    "\n",
    "#4.2 Sequence loop pattern\n",
    "def fibo(num):\n",
    "    first = 1\n",
    "    second = 1\n",
    "    while second < num:\n",
    "        first, second = second, first + second\n",
    "    return second\n",
    "#4.3 Infinite loop pattern\n",
    "while True:\n",
    "    name = input('What is your name?')\n",
    "    print('Hello {}'.format(name))\n",
    "    \n",
    "#4.4 Loop-and-a-half pattern\n",
    "def cities():\n",
    "    lst = []\n",
    "    while True:\n",
    "        city = input(\"Enter city name: \")\n",
    "        if city == '':\n",
    "            return lst # final return, will not loop anymore\n",
    "        else:\n",
    "            lst.append(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2 Function\n",
    "# Define shout_all with parameters word1 and word2\n",
    "def shout_all(word1, word2):\n",
    "    \n",
    "    # Concatenate word1 with '!!!': shout1\n",
    "    shout1 = word1 + '!!!'\n",
    "    \n",
    "    # Concatenate word2 with '!!!': shout2\n",
    "    shout2 = word2 + '!!!'\n",
    "    \n",
    "    # Construct a tuple with shout1 and shout2: shout_words\n",
    "    shout_words = (shout1, shout2)\n",
    "\n",
    "    # Return shout_words\n",
    "    return shout_words\n",
    "yell1, yell2 = shout_all('congratulations', 'you')\n",
    "\n",
    "df = pd.read_csv('tweets.csv')\n",
    "df.iloc[0]['lang']\n",
    "# Initialize an empty dictionary: langs_count\n",
    "langs_count = {}\n",
    "\n",
    "# Iterate over lang column in DataFrame\n",
    "for index, tweet in df.iterrows():\n",
    "    lang = tweet['lang']\n",
    "    #print(lang)\n",
    "    \n",
    "    # If the language is in langs_count, add 1 \n",
    "    if lang in langs_count.keys():\n",
    "        langs_count[lang] += 1\n",
    "    # Else add the language to langs_count, set the value to 1\n",
    "    else:\n",
    "        langs_count[lang] = 1\n",
    "\n",
    "# Print the populated dictionary\n",
    "print(langs_count)\n",
    "# dict[char] = dict.get(char,0) +1\n",
    "# sorted(dict.items, key = lambda x:x[1])\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Define count_entries()\n",
    "def count_entries(filename, col_name):\n",
    "    \"\"\"Return a dictionary with counts of \n",
    "    occurrences as value for each key.\"\"\"\n",
    "\n",
    "    # Import Twitter data as DataFrame: df\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "\n",
    "    # Initialize an empty dictionary: langs_count\n",
    "    counter = {}\n",
    "\n",
    "    # Iterate over lang column in DataFrame\n",
    "    for index, tweet in df.iterrows():\n",
    "        lang = tweet[col_name]\n",
    "        #print(lang)\n",
    "\n",
    "        # If the language is in langs_count, add 1 \n",
    "        if lang in counter.keys():\n",
    "            counter[lang] += 1\n",
    "        # Else add the language to langs_count, set the value to 1\n",
    "        else:\n",
    "            counter[lang] = 1\n",
    "\n",
    "    # Print the populated dictionary\n",
    "    return counter\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def count_entries(filename, col_name):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    counter = {}\n",
    "    for index, tweet in df.iterrows():\n",
    "        lang = tweet[col_name]\n",
    "\n",
    "        if lang in counter.keys():\n",
    "            counter[lang] += 1\n",
    "        else:\n",
    "            counter[lang] = 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "dict_res = count_entries('tweets.csv', 'source')\n",
    "result = sorted(dict_res, key=dict_res.get, reverse = True)\n",
    "result2 = sorted(dict_res.items(), key= lambda x:x[1], reverse = True)\n",
    "\n",
    "n_items = result2[:3]\n",
    "\n",
    "\n",
    "print (n_items)\n",
    "\n",
    "def is_prime(n):\n",
    "    res = True\n",
    "    if n == 1:\n",
    "        res = False\n",
    "    else:\n",
    "        for i in range (2,n):\n",
    "            if n % i == 0:\n",
    "                res = False\n",
    "                break\n",
    "    return res\n",
    "\n",
    "print (is_prime(31))   \n",
    "\n",
    "def find_largest_prime(n):\n",
    "    for i in range (1,n):\n",
    "        if is_prime(n-i)== True:\n",
    "            return n-i\n",
    "\n",
    "print (find_largest_prime(100))\n",
    "\n",
    "\n",
    "# Method 1\n",
    "def acronym(string):\n",
    "    lst = string.split()\n",
    "    res = ''\n",
    "    for word in lst:\n",
    "        res = (res + word[0]).upper()\n",
    "    return res\n",
    "\n",
    "print (acronym(\"GNU's not UNIX\"))\n",
    "\n",
    "\n",
    "# Method 2\n",
    "def acr (string):\n",
    "    lst = string.split()\n",
    "    return (\"\".join(list(word[0].upper() for word in lst)))\n",
    "\n",
    "print (acr(\"GNU's not UNIX\"))\n",
    "\n",
    "# method 1\n",
    "def arithmetic (lst):\n",
    "    gap = 0\n",
    "    status = True\n",
    "    if len(lst) <= 2:\n",
    "        status = True\n",
    "    else:\n",
    "        gap = lst[1]-lst[0]\n",
    "        for i in range(1,len(lst)):\n",
    "            if lst[i]-lst[i-1] != gap:\n",
    "                status = False\n",
    "                break\n",
    "    return status\n",
    "\n",
    "print (arithmetic([3, 6, 9, 11, 14]))\n",
    "\n",
    "\n",
    "# method 2\n",
    "def arit (lst):\n",
    "    if len(lst) <= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return len(set([lst[x + 1] - lst[x] for x in range(len(lst) - 1)])) == 1\n",
    "\n",
    "print (arit ([3, 6, 9, 11, 14]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chapter4 logic flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare arrays\n",
    "# Create arrays\n",
    "import numpy as np\n",
    "\n",
    "my_house = np.array([18.0, 20.0, 10.75, 9.50])\n",
    "your_house = np.array([14.0, 24.0, 14.25, 9.0])\n",
    "# my_house greater than or equal to 18\n",
    "print(my_house >= 18) # [ True  True False False]\n",
    "# my_house less than your_house\n",
    "print(my_house < your_house) #[False  True  True False]\n",
    "#Notice that not has a higher priority than and and or, it is executed first.\n",
    "# my_house greater than 18.5 or smaller than 10\n",
    "print(my_house > 18.5)\n",
    "print(my_house < 10)\n",
    "print('or')\n",
    "print(np.logical_or(my_house > 18.5, my_house < 10))\n",
    "import numpy as np\n",
    "\n",
    "# Create medium: observations with cars_per_cap between 100 and 500\n",
    "medium = cars[np.logical_and(cars['cars_per_cap'] > 100, \n",
    "                             cars['cars_per_cap'] < 500)]\n",
    "\n",
    "right = np.mean(cars[cars[\"drives_right\"]== True][\"cars_per_cap\"])\n",
    "left = np.mean(cars[cars[\"drives_right\"]== False][\"cars_per_cap\"])\n",
    "print(right,left)\n",
    "\n",
    "import time\n",
    "\n",
    "# areas list\n",
    "areas = [11.25, 18.0, 20.0, 10.75, 9.50]\n",
    "\n",
    "# Change for loop to use enumerate() and update print()\n",
    "for index, a in enumerate(areas) :\n",
    "    print(\"room {}: {}\".format(index, a))\n",
    "    time.sleep(1)\n",
    "    \n",
    "# house list of lists\n",
    "house = [[\"hallway\", 11.25], \n",
    "         [\"kitchen\", 18.0], \n",
    "         [\"living room\", 20.0], \n",
    "         [\"bedroom\", 10.75], \n",
    "         [\"bathroom\", 9.50]]\n",
    "\n",
    "for lst in house:\n",
    "    print (\"the \"+ str(lst[0]) + \" is \" + str(lst[1]) + \" sqm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop over different structures:\n",
    "#Dict\n",
    "for k, v in europe.items():\n",
    "    print('the capital of {} is {}'.format(k, v))\n",
    "#Dataframe\n",
    "for row in cars:\n",
    "    print(row) # loop over column names cars_per_cap,country,drives_right\n",
    "# Iterate over rows of cars\n",
    "for label, row in cars.iterrows():\n",
    "    print(label)\n",
    "# Adapt for loop\n",
    "for lab, row in cars.iterrows() :\n",
    "    print('{}: {}'.format(lab, row['cars_per_cap']))\n",
    "    \n",
    "    # Code for loop that adds COUNTRY column\n",
    "\n",
    "#Add new column\n",
    "for lab, row in cars.iterrows():\n",
    "    cars.loc[lab, 'COUNTRY'] = row['country'].upper()\n",
    "\n",
    "# Print cars\n",
    "cars\n",
    "\n",
    "# Use .apply(str.upper)\n",
    "cars[\"COUNTRY2\"] = cars[\"country\"].apply(str.upper)\n",
    "cars[\"COUNTRY_LENGTH\"] = cars[\"country\"].apply(len)\n",
    "\n",
    "#loop over DataFrame with multiple conditions\n",
    "import numpy as np\n",
    "cars[np.logical_and(cars[\"cars_per_cap\"]>100,cars[\"drives_right\"]==True)]\n",
    "\n",
    "for lab, row in cars.iterrows():\n",
    "    if np.logical_and(cars.loc[lab,\"cars_per_cap\"]>100, cars.loc[lab,\"drives_right\"] == True):\n",
    "        print (cars.loc[[lab]])\n",
    "        \n",
    "#loop over numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('mlb_sample.csv')\n",
    "np_height = np.array(df['Height(inches)'].values)\n",
    "np_baseball = np.array(df['Weight(pounds)'].values)\n",
    "# For loop over np_height\n",
    "for h in np.nditer(np_height):\n",
    "    print(\"{} inches\".format(h))\n",
    "# For loop over np_baseball\n",
    "for b in np.nditer(np_baseball):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter3 Matplotlib & Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1 \n",
    "#Lineplot\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('world_year_pop.csv')\n",
    "plt.plot(df1.years, df1.populations)\n",
    "plt.show()\n",
    "#Scatterplot\n",
    "plt.scatter(df2.gdp_cap, df2.life_exp)\n",
    "# Put the x-axis on a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "#histgram\n",
    "import pandas as pd\n",
    "df3 = pd.read_csv('country_life_gdp.csv')\n",
    "# Create histogram of life_exp data\n",
    "plt.hist(df3.life_exp,bins=5)\n",
    "# Display histogram\n",
    "plt.show()\n",
    "\n",
    "#Compare 2\n",
    "plt.hist(df4.life_1950, bins=15, alpha=0.4)\n",
    "plt.hist(df4.life_2007, bins=15, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2 Dictionary\n",
    "print(europe.keys())\n",
    "print(europe.values())\n",
    "# Remove australia\n",
    "del(europe['australia'])\n",
    "\n",
    "#Pandas\n",
    "cars = pd.read_csv('cars.csv', index_col=0)\n",
    "# Print out country column as Pandas Series\n",
    "cars['country']\n",
    "# Print out country column as Pandas DataFrame\n",
    "cars[['country']]\n",
    "# Print out DataFrame with country and drives_right columns\n",
    "cars[['country', 'drives_right']]\n",
    "# Print out first 3 observations\n",
    "cars[:4]\n",
    "# Print out observation for Japan as Pandas series\n",
    "cars.loc['JAP']\n",
    "# Print out observation for Japan as dataframe\n",
    "cars.loc[['JAP']]\n",
    "# Print out observations for Australia and Egypt\n",
    "cars.loc[['AUS', 'EG']]\n",
    "# Print out drives_right value of Morocco\n",
    "cars.loc['MOR', 'drives_right']\n",
    "# Print out drives_right value of Morocco as dataframe\n",
    "cars.loc[['MOR'], ['drives_right']]\n",
    "# Print sub-DataFrame\n",
    "cars.loc[['RU', 'MOR'], ['country', 'drives_right']]\n",
    "# Print out drives_right column as Series\n",
    "cars.loc[:, 'drives_right']\n",
    "# Print out drives_right column as DataFrame\n",
    "cars.loc[:, ['drives_right']]\n",
    "# Print out cars_per_cap and drives_right as DataFrame\n",
    "cars.loc[:, ['cars_per_cap', 'drives_right']]\n",
    "cars.loc[:, 'cars_per_cap'].values.mean()\n",
    "lst = [\"A\",\"E\",\"I\",\"O\",\"U\"]\n",
    "cars[cars[\"country\"].str.startswith(tuple(lst))][\"cars_per_cap\"].values.mean()\n",
    "cars.loc[[\"US\",\"AUS\",\"IN\",\"EG\"], 'cars_per_cap'].values.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
